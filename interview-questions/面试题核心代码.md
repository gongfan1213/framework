# 大模型面试核心代码示例

## 1. Transformer核心实现

### 1.1 注意力机制
```python
import torch
import torch.nn.functional as F
import math

def attention(Q, K, V, mask=None):
    """缩放点积注意力"""
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))
    
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights

class MultiHeadAttention(torch.nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = torch.nn.Linear(d_model, d_model)
        self.W_k = torch.nn.Linear(d_model, d_model)
        self.W_v = torch.nn.Linear(d_model, d_model)
        self.W_o = torch.nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 线性变换并重塑
        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 注意力计算
        output, attention_weights = attention(Q, K, V, mask)
        
        # 合并多头
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        return self.W_o(output), attention_weights
```

### 1.2 Transformer块
```python
class TransformerBlock(torch.nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.feed_forward = torch.nn.Sequential(
            torch.nn.Linear(d_model, d_ff),
            torch.nn.ReLU(),
            torch.nn.Linear(d_ff, d_model)
        )
        self.norm1 = torch.nn.LayerNorm(d_model)
        self.norm2 = torch.nn.LayerNorm(d_model)
        self.dropout = torch.nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # 自注意力 + 残差连接
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 前馈网络 + 残差连接
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x
```

## 2. LoRA实现

```python
class LoRALinear(torch.nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=32):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # 原始权重（冻结）
        self.original_weight = torch.nn.Parameter(torch.randn(out_features, in_features))
        self.original_bias = torch.nn.Parameter(torch.zeros(out_features))
        
        # LoRA权重（可训练）
        self.lora_A = torch.nn.Parameter(torch.randn(rank, in_features) * 0.02)
        self.lora_B = torch.nn.Parameter(torch.zeros(out_features, rank))
        
        # 冻结原始权重
        self.original_weight.requires_grad = False
        self.original_bias.requires_grad = False
    
    def forward(self, x):
        # 原始输出
        original_output = torch.nn.functional.linear(x, self.original_weight, self.original_bias)
        
        # LoRA输出
        lora_output = x @ self.lora_A.T @ self.lora_B.T
        lora_output = self.scaling * lora_output
        
        return original_output + lora_output
```

## 3. RLHF实现

### 3.1 奖励模型
```python
class RewardModel(torch.nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.reward_head = torch.nn.Sequential(
            torch.nn.Linear(768, 768),
            torch.nn.ReLU(),
            torch.nn.Linear(768, 1)
        )
    
    def forward(self, input_ids, attention_mask=None):
        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        reward = self.reward_head(pooled_output)
        return reward

class RewardTrainer:
    def __init__(self, reward_model):
        self.reward_model = reward_model
        self.optimizer = torch.optim.AdamW(reward_model.parameters(), lr=1e-5)
    
    def train_step(self, chosen_ids, rejected_ids, chosen_mask=None, rejected_mask=None):
        # 计算chosen和rejected的奖励
        chosen_rewards = self.reward_model(chosen_ids, chosen_mask)
        rejected_rewards = self.reward_model(rejected_ids, rejected_mask)
        
        # 对比损失
        loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
```

### 3.2 PPO训练
```python
class PPOTrainer:
    def __init__(self, policy_model, value_model, reward_model):
        self.policy_model = policy_model
        self.value_model = value_model
        self.reward_model = reward_model
        self.policy_optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)
        self.value_optimizer = torch.optim.AdamW(value_model.parameters(), lr=1e-5)
        
        self.clip_epsilon = 0.2
        self.value_coef = 0.5
        self.entropy_coef = 0.01
    
    def compute_advantages(self, rewards, values, gamma=0.99, lam=0.95):
        """计算优势函数"""
        advantages = torch.zeros_like(rewards)
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value - values[t]
            advantages[t] = delta + gamma * lam * last_advantage
            last_advantage = advantages[t]
        
        return advantages
    
    def train_step(self, states, actions, old_log_probs, rewards):
        # 计算价值
        values = self.value_model(states)
        
        # 计算优势
        advantages = self.compute_advantages(rewards, values)
        returns = advantages + values
        
        # 标准化优势
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # 计算新的动作概率
        log_probs = self.policy_model.get_log_probs(states, actions)
        ratio = torch.exp(log_probs - old_log_probs)
        
        # PPO损失
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # 价值损失
        value_loss = torch.nn.functional.mse_loss(values, returns)
        
        # 总损失
        total_loss = policy_loss + self.value_coef * value_loss
        
        self.policy_optimizer.zero_grad()
        self.value_optimizer.zero_grad()
        total_loss.backward()
        self.policy_optimizer.step()
        self.value_optimizer.step()
        
        return total_loss.item()
```

## 4. 模型量化

```python
import torch.quantization

class ModelQuantizer:
    def __init__(self, model):
        self.model = model
        self.quantized_model = None
    
    def dynamic_quantization(self):
        """动态量化"""
        self.quantized_model = torch.quantization.quantize_dynamic(
            self.model,
            {torch.nn.Linear, torch.nn.LSTM},
            dtype=torch.qint8
        )
        return self.quantized_model
    
    def static_quantization(self, calibration_data):
        """静态量化"""
        # 准备量化
        self.model.eval()
        self.model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # 融合操作
        torch.quantization.prepare(self.model, inplace=True)
        
        # 校准
        with torch.no_grad():
            for data in calibration_data:
                self.model(data)
        
        # 转换为量化模型
        self.quantized_model = torch.quantization.convert(self.model, inplace=False)
        return self.quantized_model
```

## 5. KV缓存优化

```python
class KVCache:
    def __init__(self, max_length=2048, num_layers=12, num_heads=12, head_dim=64):
        self.max_length = max_length
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.head_dim = head_dim
        
        # 初始化缓存
        self.cache = {
            'key': torch.zeros(num_layers, max_length, num_heads, head_dim),
            'value': torch.zeros(num_layers, max_length, num_heads, head_dim)
        }
        self.current_length = 0
    
    def update_cache(self, layer_idx, key, value, position_ids):
        """更新缓存"""
        for i, pos in enumerate(position_ids):
            if pos < self.max_length:
                self.cache['key'][layer_idx, pos] = key[i]
                self.cache['value'][layer_idx, pos] = value[i]
        
        self.current_length = max(self.current_length, position_ids.max().item() + 1)
    
    def get_cache(self, layer_idx, start_pos, end_pos):
        """获取缓存"""
        return (
            self.cache['key'][layer_idx, start_pos:end_pos],
            self.cache['value'][layer_idx, start_pos:end_pos]
        )
    
    def clear_cache(self):
        """清空缓存"""
        self.cache['key'].zero_()
        self.cache['value'].zero_()
        self.current_length = 0
```

## 6. RAG系统

```python
class RAGSystem:
    def __init__(self, llm, retriever, documents):
        self.llm = llm
        self.retriever = retriever
        self.documents = documents
    
    def retrieve(self, query, top_k=5):
        """检索相关文档"""
        # 计算查询嵌入
        query_embedding = self.retriever.encode(query)
        
        # 计算相似度
        similarities = []
        for doc in self.documents:
            doc_embedding = self.retriever.encode(doc['content'])
            similarity = torch.cosine_similarity(
                query_embedding.unsqueeze(0), 
                doc_embedding.unsqueeze(0)
            ).item()
            similarities.append((similarity, doc))
        
        # 排序并返回top_k
        similarities.sort(key=lambda x: x[0], reverse=True)
        return [doc for _, doc in similarities[:top_k]]
    
    def answer(self, question):
        """回答问题"""
        # 检索相关文档
        relevant_docs = self.retrieve(question)
        
        # 构建上下文
        context = "\n".join([doc['content'] for doc in relevant_docs])
        
        # 构建提示
        prompt = f"""基于以下文档回答问题：

文档：
{context}

问题：{question}

答案："""
        
        # 生成答案
        answer = self.llm.generate(prompt)
        return answer
```

## 7. 多智能体系统

```python
class Agent:
    def __init__(self, agent_id, capabilities):
        self.agent_id = agent_id
        self.capabilities = capabilities
        self.memory = []
    
    def can_handle(self, task):
        """检查是否能处理任务"""
        return any(cap in task for cap in self.capabilities)
    
    def execute(self, task):
        """执行任务"""
        if self.can_handle(task):
            result = f"Agent {self.agent_id} 处理任务: {task}"
            self.memory.append((task, result))
            return result
        else:
            return f"Agent {self.agent_id} 无法处理任务: {task}"

class MultiAgentSystem:
    def __init__(self):
        self.agents = {}
        self.message_queue = {}
    
    def add_agent(self, agent_id, agent):
        self.agents[agent_id] = agent
        self.message_queue[agent_id] = []
    
    def send_message(self, from_agent, to_agent, message):
        """发送消息"""
        if to_agent in self.message_queue:
            self.message_queue[to_agent].append({
                'from': from_agent,
                'content': message,
                'timestamp': time.time()
            })
    
    def execute_task(self, task):
        """执行任务"""
        # 找到能处理任务的智能体
        suitable_agents = [
            agent for agent in self.agents.values() 
            if agent.can_handle(task)
        ]
        
        if not suitable_agents:
            return f"没有智能体能处理任务: {task}"
        
        # 选择第一个合适的智能体
        selected_agent = suitable_agents[0]
        result = selected_agent.execute(task)
        
        return result
```

## 8. 监控系统

```python
class MonitoringSystem:
    def __init__(self):
        self.metrics = {
            'request_count': 0,
            'error_count': 0,
            'avg_response_time': 0,
            'total_response_time': 0
        }
        self.start_time = time.time()
    
    def record_request(self, response_time, success):
        """记录请求"""
        self.metrics['request_count'] += 1
        self.metrics['total_response_time'] += response_time
        
        if not success:
            self.metrics['error_count'] += 1
        
        # 更新平均响应时间
        self.metrics['avg_response_time'] = (
            self.metrics['total_response_time'] / self.metrics['request_count']
        )
    
    def get_metrics(self):
        """获取指标"""
        uptime = time.time() - self.start_time
        error_rate = (
            self.metrics['error_count'] / self.metrics['request_count']
            if self.metrics['request_count'] > 0 else 0
        )
        
        return {
            **self.metrics,
            'uptime': uptime,
            'error_rate': error_rate,
            'requests_per_second': self.metrics['request_count'] / uptime
        }
```

## 面试要点总结

### 技术要点
1. **Transformer架构**：注意力机制、位置编码、多头注意力
2. **预训练微调**：LoRA、指令微调、参数高效微调
3. **强化学习**：RLHF、PPO、奖励建模
4. **工程优化**：模型量化、KV缓存、性能优化
5. **系统设计**：RAG、多智能体、监控系统

### 代码能力
- 能够手写核心算法
- 理解实现细节
- 考虑性能优化
- 处理边界情况

### 系统思维
- 架构设计能力
- 性能分析能力
- 问题解决能力
- 工程实践能力

这些代码示例涵盖了大模型面试的核心技术点，建议深入理解每个实现，并能够解释其原理和优化思路。
