# 大模型面试题详解

## 1. 基础理论面试题

### 1.1 Transformer架构详解

**Q: 请详细解释Transformer的编码器和解码器结构，并实现一个简化版本？**

**A:** Transformer由编码器和解码器组成，每个都有多层结构。

```python
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 线性变换
        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 注意力计算
        output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # 合并多头
        output = output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        return self.W_o(output), attention_weights

class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # 自注意力 + 残差连接
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 前馈网络 + 残差连接
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class Transformer(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, max_len=5000, dropout=0.1):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        
        self.encoder_layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # 词嵌入 + 位置编码
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # 编码器层
        for encoder_layer in self.encoder_layers:
            x = encoder_layer(x, mask)
        
        return x
```

### 1.2 注意力机制深度解析

**Q: 请解释为什么需要缩放点积注意力？多头注意力的优势是什么？**

**A:** 

**缩放点积注意力的原因：**
1. **梯度稳定性**：当d_k较大时，点积值会变得很大，导致softmax梯度接近0
2. **数值稳定性**：缩放后避免数值溢出

**多头注意力的优势：**
1. **并行计算**：多个头可以并行计算
2. **不同表示子空间**：每个头关注不同的特征模式
3. **增强表达能力**：组合多个子空间的信息

```python
def attention_analysis():
    # 演示缩放的重要性
    d_k = 64
    Q = torch.randn(1, 10, d_k)
    K = torch.randn(1, 10, d_k)
    
    # 未缩放的点积
    scores_unscaled = torch.matmul(Q, K.transpose(-2, -1))
    print(f"未缩放分数范围: {scores_unscaled.min():.2f} ~ {scores_unscaled.max():.2f}")
    
    # 缩放的点积
    scores_scaled = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    print(f"缩放后分数范围: {scores_scaled.min():.2f} ~ {scores_scaled.max():.2f}")
    
    # 比较softmax输出
    softmax_unscaled = torch.softmax(scores_unscaled, dim=-1)
    softmax_scaled = torch.softmax(scores_scaled, dim=-1)
    
    print(f"未缩放softmax熵: {-(softmax_unscaled * torch.log(softmax_unscaled + 1e-8)).sum():.4f}")
    print(f"缩放softmax熵: {-(softmax_scaled * torch.log(softmax_scaled + 1e-8)).sum():.4f}")
```

## 2. 预训练与微调面试题

### 2.1 LoRA实现详解

**Q: 请详细实现LoRA（Low-Rank Adaptation）算法？**

**A:** LoRA通过低秩矩阵分解来减少可训练参数数量。

```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=32, dropout=0.1):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # 原始权重（冻结）
        self.original_weight = nn.Parameter(torch.randn(out_features, in_features))
        self.original_bias = nn.Parameter(torch.zeros(out_features))
        
        # LoRA权重（可训练）
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.02)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        self.lora_dropout = nn.Dropout(dropout)
        
        # 冻结原始权重
        self.original_weight.requires_grad = False
        self.original_bias.requires_grad = False
    
    def forward(self, x):
        # 原始输出
        original_output = F.linear(x, self.original_weight, self.original_bias)
        
        # LoRA输出
        lora_output = self.lora_dropout(x) @ self.lora_A.T @ self.lora_B.T
        lora_output = self.scaling * lora_output
        
        return original_output + lora_output

class LoRATransformer(nn.Module):
    def __init__(self, base_model, rank=16, alpha=32):
        super().__init__()
        self.base_model = base_model
        
        # 为每个线性层添加LoRA
        for name, module in self.base_model.named_modules():
            if isinstance(module, nn.Linear):
                # 替换为LoRA层
                lora_layer = LoRALinear(
                    module.in_features,
                    module.out_features,
                    rank=rank,
                    alpha=alpha
                )
                # 复制原始权重
                lora_layer.original_weight.data = module.weight.data.clone()
                if module.bias is not None:
                    lora_layer.original_bias.data = module.bias.data.clone()
                
                # 替换模块
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                parent = self.base_model.get_submodule(parent_name)
                setattr(parent, child_name, lora_layer)
    
    def forward(self, *args, **kwargs):
        return self.base_model(*args, **kwargs)
    
    def save_lora_weights(self, path):
        """保存LoRA权重"""
        lora_weights = {}
        for name, module in self.named_modules():
            if isinstance(module, LoRALinear):
                lora_weights[f"{name}.lora_A"] = module.lora_A.data
                lora_weights[f"{name}.lora_B"] = module.lora_B.data
        torch.save(lora_weights, path)
    
    def load_lora_weights(self, path):
        """加载LoRA权重"""
        lora_weights = torch.load(path)
        for name, module in self.named_modules():
            if isinstance(module, LoRALinear):
                if f"{name}.lora_A" in lora_weights:
                    module.lora_A.data = lora_weights[f"{name}.lora_A"]
                if f"{name}.lora_B" in lora_weights:
                    module.lora_B.data = lora_weights[f"{name}.lora_B"]
```

### 2.2 指令微调实现

**Q: 请实现指令微调的训练流程？**

**A:** 指令微调通过构造指令-响应对来训练模型。

```python
class InstructionTuningDataset:
    def __init__(self, data_path):
        self.data = self.load_data(data_path)
    
    def load_data(self, data_path):
        # 加载指令数据
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    
    def format_instruction(self, instruction, input_text=None, output_text=None):
        """格式化指令"""
        if input_text:
            prompt = f"指令：{instruction}\n输入：{input_text}\n输出："
        else:
            prompt = f"指令：{instruction}\n输出："
        
        if output_text:
            return prompt + output_text
        else:
            return prompt
    
    def __getitem__(self, idx):
        item = self.data[idx]
        instruction = item['instruction']
        input_text = item.get('input', '')
        output_text = item['output']
        
        formatted_text = self.format_instruction(instruction, input_text, output_text)
        
        return {
            'text': formatted_text,
            'instruction': instruction,
            'input': input_text,
            'output': output_text
        }
    
    def __len__(self):
        return len(self.data)

class InstructionTuningTrainer:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
    def train_step(self, batch):
        # 编码输入
        inputs = self.tokenizer(
            batch['text'],
            truncation=True,
            padding=True,
            return_tensors='pt'
        ).to(self.device)
        
        # 计算损失
        outputs = self.model(**inputs, labels=inputs['input_ids'])
        loss = outputs.loss
        
        return loss
    
    def generate_response(self, instruction, input_text=None, max_length=100):
        # 格式化指令
        if input_text:
            prompt = f"指令：{instruction}\n输入：{input_text}\n输出："
        else:
            prompt = f"指令：{instruction}\n输出："
        
        # 编码
        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)
        
        # 生成
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # 解码
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # 提取输出部分
        response = response[len(prompt):]
        
        return response
```

## 3. 强化学习面试题

### 3.1 PPO算法实现

**Q: 请实现PPO（Proximal Policy Optimization）算法？**

**A:** PPO是一种重要的强化学习算法，用于策略优化。

```python
class PPOTrainer:
    def __init__(self, policy_model, value_model, reward_model, device):
        self.policy_model = policy_model
        self.value_model = value_model
        self.reward_model = reward_model
        self.device = device
        
        self.policy_optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)
        self.value_optimizer = torch.optim.AdamW(value_model.parameters(), lr=1e-5)
        
        self.clip_epsilon = 0.2
        self.value_coef = 0.5
        self.entropy_coef = 0.01
        
    def compute_advantages(self, rewards, values, gamma=0.99, lam=0.95):
        """计算优势函数"""
        advantages = torch.zeros_like(rewards)
        last_advantage = 0
        last_value = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = last_value
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value - values[t]
            advantages[t] = delta + gamma * lam * last_advantage
            last_advantage = advantages[t]
        
        return advantages
    
    def compute_policy_loss(self, states, actions, old_log_probs, advantages, clip_epsilon=0.2):
        """计算策略损失"""
        # 计算新的动作概率
        log_probs = self.policy_model.get_log_probs(states, actions)
        
        # 计算概率比率
        ratio = torch.exp(log_probs - old_log_probs)
        
        # 计算裁剪后的目标
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages
        
        # 策略损失
        policy_loss = -torch.min(surr1, surr2).mean()
        
        return policy_loss
    
    def compute_value_loss(self, states, returns):
        """计算价值损失"""
        values = self.value_model(states)
        value_loss = F.mse_loss(values, returns)
        return value_loss
    
    def compute_entropy_loss(self, states):
        """计算熵损失（用于探索）"""
        log_probs = self.policy_model.get_log_probs(states)
        entropy = -(log_probs * torch.exp(log_probs)).sum(dim=-1).mean()
        return -entropy
    
    def train_step(self, batch):
        states = batch['states'].to(self.device)
        actions = batch['actions'].to(self.device)
        rewards = batch['rewards'].to(self.device)
        old_log_probs = batch['log_probs'].to(self.device)
        
        # 计算价值
        values = self.value_model(states)
        
        # 计算优势
        advantages = self.compute_advantages(rewards, values)
        returns = advantages + values
        
        # 标准化优势
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # 策略损失
        policy_loss = self.compute_policy_loss(
            states, actions, old_log_probs, advantages, self.clip_epsilon
        )
        
        # 价值损失
        value_loss = self.compute_value_loss(states, returns)
        
        # 熵损失
        entropy_loss = self.compute_entropy_loss(states)
        
        # 总损失
        total_loss = (
            policy_loss + 
            self.value_coef * value_loss + 
            self.entropy_coef * entropy_loss
        )
        
        # 反向传播
        self.policy_optimizer.zero_grad()
        self.value_optimizer.zero_grad()
        total_loss.backward()
        self.policy_optimizer.step()
        self.value_optimizer.step()
        
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy_loss': entropy_loss.item(),
            'total_loss': total_loss.item()
        }
```

### 3.2 奖励建模实现

**Q: 请实现奖励建模的训练过程？**

**A:** 奖励建模通过人类偏好数据训练奖励模型。

```python
class RewardModel(nn.Module):
    def __init__(self, base_model, hidden_size=768):
        super().__init__()
        self.base_model = base_model
        self.reward_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )
    
    def forward(self, input_ids, attention_mask=None):
        # 获取隐藏状态
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # 使用最后一层的[CLS]标记或平均池化
        hidden_states = outputs.hidden_states[-1]
        
        if attention_mask is not None:
            # 平均池化
            mask_expanded = attention_mask.unsqueeze(-1).float()
            pooled_output = (hidden_states * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1)
        else:
            # 使用第一个token
            pooled_output = hidden_states[:, 0, :]
        
        # 计算奖励
        reward = self.reward_head(pooled_output)
        
        return reward

class RewardModelTrainer:
    def __init__(self, reward_model, device):
        self.reward_model = reward_model
        self.device = device
        self.optimizer = torch.optim.AdamW(reward_model.parameters(), lr=1e-5)
    
    def train_step(self, chosen_ids, rejected_ids, chosen_mask=None, rejected_mask=None):
        # 计算chosen的奖励
        chosen_rewards = self.reward_model(chosen_ids, chosen_mask)
        
        # 计算rejected的奖励
        rejected_rewards = self.reward_model(rejected_ids, rejected_mask)
        
        # 计算对比损失
        loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()
        
        # 反向传播
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def predict_reward(self, input_ids, attention_mask=None):
        """预测奖励值"""
        self.reward_model.eval()
        with torch.no_grad():
            reward = self.reward_model(input_ids, attention_mask)
        return reward.item()
```

## 4. 工程实践面试题

### 4.1 模型量化实现

**Q: 请实现动态量化和静态量化？**

**A:** 模型量化是减少模型大小和加速推理的重要技术。

```python
class ModelQuantizer:
    def __init__(self, model):
        self.model = model
        self.quantized_model = None
    
    def dynamic_quantization(self):
        """动态量化"""
        self.quantized_model = torch.quantization.quantize_dynamic(
            self.model,
            {nn.Linear, nn.LSTM, nn.LSTMCell, nn.RNNCell, nn.GRUCell},
            dtype=torch.qint8
        )
        return self.quantized_model
    
    def static_quantization(self, calibration_data):
        """静态量化"""
        # 准备量化
        self.model.eval()
        self.model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # 融合操作
        torch.quantization.prepare(self.model, inplace=True)
        
        # 校准
        self._calibrate_model(calibration_data)
        
        # 转换为量化模型
        self.quantized_model = torch.quantization.convert(self.model, inplace=False)
        
        return self.quantized_model
    
    def _calibrate_model(self, calibration_data):
        """校准模型"""
        with torch.no_grad():
            for data in calibration_data:
                self.model(data)
    
    def compare_performance(self, test_data):
        """比较量化前后的性能"""
        # 原始模型
        self.model.eval()
        start_time = time.time()
        with torch.no_grad():
            for data in test_data:
                _ = self.model(data)
        original_time = time.time() - start_time
        
        # 量化模型
        self.quantized_model.eval()
        start_time = time.time()
        with torch.no_grad():
            for data in test_data:
                _ = self.quantized_model(data)
        quantized_time = time.time() - start_time
        
        # 模型大小比较
        original_size = sum(p.numel() * p.element_size() for p in self.model.parameters())
        quantized_size = sum(p.numel() * p.element_size() for p in self.quantized_model.parameters())
        
        return {
            'original_time': original_time,
            'quantized_time': quantized_time,
            'speedup': original_time / quantized_time,
            'original_size': original_size,
            'quantized_size': quantized_size,
            'size_reduction': (original_size - quantized_size) / original_size
        }
```

### 4.2 KV缓存优化

**Q: 请实现KV缓存机制来优化推理性能？**

**A:** KV缓存是Transformer推理优化的重要技术。

```python
class KVCache:
    def __init__(self, max_length=2048, num_layers=12, num_heads=12, head_dim=64):
        self.max_length = max_length
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.head_dim = head_dim
        
        # 初始化缓存
        self.cache = {
            'key': torch.zeros(num_layers, max_length, num_heads, head_dim),
            'value': torch.zeros(num_layers, max_length, num_heads, head_dim)
        }
        self.current_length = 0
    
    def update_cache(self, layer_idx, key, value, position_ids):
        """更新缓存"""
        # 将新的key和value添加到缓存中
        for i, pos in enumerate(position_ids):
            if pos < self.max_length:
                self.cache['key'][layer_idx, pos] = key[i]
                self.cache['value'][layer_idx, pos] = value[i]
        
        self.current_length = max(self.current_length, position_ids.max().item() + 1)
    
    def get_cache(self, layer_idx, start_pos, end_pos):
        """获取缓存"""
        return (
            self.cache['key'][layer_idx, start_pos:end_pos],
            self.cache['value'][layer_idx, start_pos:end_pos]
        )
    
    def clear_cache(self):
        """清空缓存"""
        self.cache['key'].zero_()
        self.cache['value'].zero_()
        self.current_length = 0

class OptimizedTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_layers = n_layers
        self.head_dim = d_model // n_heads
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        
        self.layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff)
            for _ in range(n_layers)
        ])
        
        self.kv_cache = KVCache(max_length=2048, num_layers=n_layers, 
                               num_heads=n_heads, head_dim=self.head_dim)
    
    def forward(self, input_ids, use_cache=True, past_key_values=None):
        batch_size, seq_len = input_ids.shape
        
        # 词嵌入
        x = self.embedding(input_ids) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        
        # 生成位置ID
        if past_key_values is not None:
            position_ids = torch.arange(past_key_values[0].size(1), 
                                      past_key_values[0].size(1) + seq_len, 
                                      device=input_ids.device)
        else:
            position_ids = torch.arange(seq_len, device=input_ids.device)
        
        # 逐层处理
        for layer_idx, layer in enumerate(self.layers):
            if use_cache and past_key_values is not None:
                # 使用缓存的KV
                past_key, past_value = past_key_values[layer_idx]
                x = layer.forward_with_cache(x, past_key, past_value, position_ids)
            else:
                # 正常前向传播
                x = layer(x)
        
        return x
    
    def generate(self, input_ids, max_length=100, temperature=1.0):
        """使用KV缓存的生成"""
        self.kv_cache.clear_cache()
        
        generated = input_ids.clone()
        current_length = input_ids.size(1)
        
        for _ in range(max_length - current_length):
            # 前向传播
            outputs = self.forward(generated, use_cache=True)
            
            # 获取下一个token的概率
            logits = outputs[:, -1, :] / temperature
            probs = torch.softmax(logits, dim=-1)
            
            # 采样下一个token
            next_token = torch.multinomial(probs, 1)
            
            # 添加到生成序列
            generated = torch.cat([generated, next_token], dim=1)
        
        return generated
```

## 5. 系统设计面试题

### 5.1 大模型服务架构

**Q: 请设计一个高并发的大模型服务架构？**

**A:** 大模型服务需要考虑负载均衡、缓存、监控等多个方面。

```python
import asyncio
import aiohttp
from typing import Dict, List, Optional
import time
import logging

class ModelService:
    def __init__(self, model_path: str, max_batch_size: int = 8):
        self.model = self.load_model(model_path)
        self.max_batch_size = max_batch_size
        self.request_queue = asyncio.Queue()
        self.processing = False
        
    def load_model(self, model_path: str):
        """加载模型"""
        # 实际实现中加载模型
        return None
    
    async def process_request(self, request: Dict):
        """处理单个请求"""
        try:
            # 模型推理
            result = await self.inference(request['input'])
            return {
                'request_id': request['request_id'],
                'result': result,
                'status': 'success'
            }
        except Exception as e:
            logging.error(f"处理请求失败: {e}")
            return {
                'request_id': request['request_id'],
                'error': str(e),
                'status': 'error'
            }
    
    async def inference(self, input_text: str):
        """模型推理"""
        # 实际实现中进行推理
        await asyncio.sleep(0.1)  # 模拟推理时间
        return f"处理结果: {input_text}"
    
    async def batch_processor(self):
        """批处理处理器"""
        while True:
            batch = []
            
            # 收集批次
            try:
                # 等待第一个请求
                first_request = await asyncio.wait_for(
                    self.request_queue.get(), timeout=0.1
                )
                batch.append(first_request)
                
                # 收集更多请求直到达到批次大小或超时
                while len(batch) < self.max_batch_size:
                    try:
                        request = await asyncio.wait_for(
                            self.request_queue.get(), timeout=0.01
                        )
                        batch.append(request)
                    except asyncio.TimeoutError:
                        break
                
                # 批量处理
                tasks = [self.process_request(req) for req in batch]
                results = await asyncio.gather(*tasks)
                
                # 返回结果
                for result in results:
                    if 'request_id' in result:
                        # 这里应该将结果发送回对应的客户端
                        pass
                        
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logging.error(f"批处理错误: {e}")

class LoadBalancer:
    def __init__(self, services: List[ModelService]):
        self.services = services
        self.current_index = 0
        self.health_checks = {}
    
    def get_next_service(self) -> Optional[ModelService]:
        """获取下一个可用服务"""
        for _ in range(len(self.services)):
            service = self.services[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.services)
            
            if self.is_healthy(service):
                return service
        
        return None
    
    def is_healthy(self, service: ModelService) -> bool:
        """健康检查"""
        # 实际实现中进行健康检查
        return True

class CacheManager:
    def __init__(self, max_size: int = 10000):
        self.cache = {}
        self.max_size = max_size
        self.access_times = {}
    
    def get(self, key: str):
        """获取缓存"""
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        return None
    
    def set(self, key: str, value, ttl: int = 3600):
        """设置缓存"""
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        self.cache[key] = value
        self.access_times[key] = time.time()
    
    def _evict_oldest(self):
        """淘汰最旧的缓存"""
        oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        del self.cache[oldest_key]
        del self.access_times[oldest_key]

class MonitoringSystem:
    def __init__(self):
        self.metrics = {
            'request_count': 0,
            'error_count': 0,
            'avg_response_time': 0,
            'active_connections': 0
        }
        self.start_time = time.time()
    
    def record_request(self, response_time: float, success: bool):
        """记录请求指标"""
        self.metrics['request_count'] += 1
        if not success:
            self.metrics['error_count'] += 1
        
        # 更新平均响应时间
        total_requests = self.metrics['request_count']
        current_avg = self.metrics['avg_response_time']
        self.metrics['avg_response_time'] = (
            (current_avg * (total_requests - 1) + response_time) / total_requests
        )
    
    def get_metrics(self) -> Dict:
        """获取监控指标"""
        uptime = time.time() - self.start_time
        error_rate = (
            self.metrics['error_count'] / self.metrics['request_count']
            if self.metrics['request_count'] > 0 else 0
        )
        
        return {
            **self.metrics,
            'uptime': uptime,
            'error_rate': error_rate,
            'requests_per_second': self.metrics['request_count'] / uptime
        }

class ModelServiceAPI:
    def __init__(self, services: List[ModelService]):
        self.load_balancer = LoadBalancer(services)
        self.cache = CacheManager()
        self.monitoring = MonitoringSystem()
    
    async def handle_request(self, request_data: Dict):
        """处理API请求"""
        start_time = time.time()
        
        try:
            # 检查缓存
            cache_key = self._generate_cache_key(request_data)
            cached_result = self.cache.get(cache_key)
            if cached_result:
                return cached_result
            
            # 获取服务
            service = self.load_balancer.get_next_service()
            if not service:
                raise Exception("No available service")
            
            # 处理请求
            result = await service.process_request(request_data)
            
            # 缓存结果
            if result['status'] == 'success':
                self.cache.set(cache_key, result)
            
            # 记录指标
            response_time = time.time() - start_time
            self.monitoring.record_request(response_time, result['status'] == 'success')
            
            return result
            
        except Exception as e:
            response_time = time.time() - start_time
            self.monitoring.record_request(response_time, False)
            raise e
    
    def _generate_cache_key(self, request_data: Dict) -> str:
        """生成缓存键"""
        import hashlib
        content = str(request_data)
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_health_status(self) -> Dict:
        """获取健康状态"""
        return {
            'status': 'healthy',
            'metrics': self.monitoring.get_metrics(),
            'cache_stats': {
                'size': len(self.cache.cache),
                'hit_rate': 0.8  # 实际实现中计算命中率
            }
        }
```

这个面试题详解包含了：

1. **基础理论**：Transformer架构、注意力机制的深度实现
2. **预训练微调**：LoRA、指令微调的完整实现
3. **强化学习**：PPO、奖励建模的详细代码
4. **工程实践**：模型量化、KV缓存优化
5. **系统设计**：高并发服务架构设计

每个部分都包含了：
- 详细的代码实现
- 核心原理解释
- 实际应用场景
- 性能优化考虑

这些内容涵盖了大模型岗位面试的主要技术点，可以帮助您全面准备面试。
