# 经典强化学习算法

## 1. 动态规划方法

### 1.1 策略迭代（Policy Iteration）

**算法思想**：
策略迭代通过交替进行策略评估和策略改进来找到最优策略。

**算法步骤**：
1. **策略评估**：计算当前策略的价值函数
2. **策略改进**：基于价值函数改进策略
3. **重复**：直到策略收敛

**策略评估**：
```python
def policy_evaluation(V, policy, P, R, gamma, theta):
    while True:
        delta = 0
        for s in states:
            v = V[s]
            V[s] = sum(P[s][policy[s]][s'] * (R[s][policy[s]][s'] + gamma * V[s']) 
                      for s' in states)
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break
    return V
```

**策略改进**：
```python
def policy_improvement(V, policy, P, R, gamma):
    policy_stable = True
    for s in states:
        old_action = policy[s]
        policy[s] = argmax_a(sum(P[s][a][s'] * (R[s][a][s'] + gamma * V[s']) 
                                for s' in states))
        if old_action != policy[s]:
            policy_stable = False
    return policy, policy_stable
```

### 1.2 值迭代（Value Iteration）

**算法思想**：
值迭代直接优化价值函数，然后从最优价值函数提取最优策略。

**算法步骤**：
1. 初始化价值函数
2. 迭代更新价值函数
3. 从最优价值函数提取策略

**算法实现**：
```python
def value_iteration(V, P, R, gamma, theta):
    while True:
        delta = 0
        for s in states:
            v = V[s]
            V[s] = max(sum(P[s][a][s'] * (R[s][a][s'] + gamma * V[s']) 
                          for s' in states) for a in actions)
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break
    
    # 提取最优策略
    policy = {}
    for s in states:
        policy[s] = argmax_a(sum(P[s][a][s'] * (R[s][a][s'] + gamma * V[s']) 
                                for s' in states))
    return V, policy
```

### 1.3 动态规划的优缺点

**优点**：
- 保证收敛到最优解
- 理论基础扎实
- 适用于小规模问题

**缺点**：
- 需要完整的环境模型
- 计算复杂度高
- 不适用于大规模问题

## 2. 蒙特卡洛方法

### 2.1 蒙特卡洛策略评估

**算法思想**：
通过采样完整的episode来估计价值函数。

**算法步骤**：
1. 使用策略π生成episode
2. 计算每个状态的回报
3. 更新价值函数估计

**首次访问MC**：
```python
def first_visit_mc_policy_evaluation(policy, episodes, gamma):
    V = defaultdict(float)
    returns = defaultdict(list)
    
    for episode in episodes:
        states, rewards = generate_episode(policy)
        G = 0
        for t in range(len(states)-1, -1, -1):
            G = gamma * G + rewards[t]
            if states[t] not in states[:t]:  # 首次访问
                returns[states[t]].append(G)
                V[states[t]] = np.mean(returns[states[t]])
    
    return V
```

### 2.2 蒙特卡洛控制

**算法思想**：
结合策略评估和策略改进，使用ε-贪婪策略进行探索。

**算法实现**：
```python
def mc_control(episodes, gamma, epsilon):
    Q = defaultdict(lambda: np.zeros(len(actions)))
    returns = defaultdict(lambda: defaultdict(list))
    
    for episode in episodes:
        states, actions, rewards = generate_episode_epsilon_greedy(Q, epsilon)
        G = 0
        for t in range(len(states)-1, -1, -1):
            G = gamma * G + rewards[t]
            if (states[t], actions[t]) not in zip(states[:t], actions[:t]):
                returns[states[t]][actions[t]].append(G)
                Q[states[t]][actions[t]] = np.mean(returns[states[t]][actions[t]])
    
    return Q
```

### 2.3 蒙特卡洛方法的优缺点

**优点**：
- 不需要环境模型
- 可以处理非马尔可夫问题
- 收敛到最优策略

**缺点**：
- 需要完整的episode
- 样本效率较低
- 不适用于连续任务

## 3. 时序差分学习

### 3.1 TD(0)学习

**算法思想**：
使用一步预测来更新价值函数估计。

**更新规则**：
```
V(s_t) ← V(s_t) + α[r_{t+1} + γV(s_{t+1}) - V(s_t)]
```

**算法实现**：
```python
def td_learning(episodes, alpha, gamma):
    V = defaultdict(float)
    
    for episode in episodes:
        state = env.reset()
        done = False
        while not done:
            action = policy(state)
            next_state, reward, done, _ = env.step(action)
            
            # TD更新
            V[state] += alpha * (reward + gamma * V[next_state] - V[state])
            state = next_state
    
    return V
```

### 3.2 SARSA算法

**算法思想**：
在线的时序差分控制算法，使用当前策略选择行动。

**更新规则**：
```
Q(s_t, a_t) ← Q(s_t, a_t) + α[r_{t+1} + γQ(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
```

**算法实现**：
```python
def sarsa(episodes, alpha, gamma, epsilon):
    Q = defaultdict(lambda: np.zeros(len(actions)))
    
    for episode in episodes:
        state = env.reset()
        action = epsilon_greedy(Q, state, epsilon)
        done = False
        
        while not done:
            next_state, reward, done, _ = env.step(action)
            next_action = epsilon_greedy(Q, next_state, epsilon)
            
            # SARSA更新
            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])
            
            state, action = next_state, next_action
    
    return Q
```

### 3.3 Q-Learning算法

**算法思想**：
离线的时序差分控制算法，使用最优策略选择下一行动。

**更新规则**：
```
Q(s_t, a_t) ← Q(s_t, a_t) + α[r_{t+1} + γ max_a Q(s_{t+1}, a) - Q(s_t, a_t)]
```

**算法实现**：
```python
def q_learning(episodes, alpha, gamma, epsilon):
    Q = defaultdict(lambda: np.zeros(len(actions)))
    
    for episode in episodes:
        state = env.reset()
        done = False
        
        while not done:
            action = epsilon_greedy(Q, state, epsilon)
            next_state, reward, done, _ = env.step(action)
            
            # Q-Learning更新
            Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])
            state = next_state
    
    return Q
```

### 3.4 时序差分方法的优缺点

**优点**：
- 在线学习，不需要完整episode
- 样本效率较高
- 可以处理连续任务

**缺点**：
- 可能不收敛（在函数逼近情况下）
- 对超参数敏感
- 探索策略影响性能

## 4. 策略梯度方法

### 4.1 REINFORCE算法

**算法思想**：
直接优化策略参数，使用策略梯度定理。

**策略梯度定理**：
```
∇J(θ) = E[∇log π(a|s,θ) * G_t]
```

**算法实现**：
```python
def reinforce(episodes, alpha, gamma):
    theta = np.random.randn(n_features)
    
    for episode in episodes:
        states, actions, rewards = generate_episode(theta)
        G = 0
        
        for t in range(len(states)-1, -1, -1):
            G = gamma * G + rewards[t]
            # 策略梯度更新
            grad = compute_policy_gradient(states[t], actions[t], theta)
            theta += alpha * gamma**t * G * grad
    
    return theta
```

### 4.2 Actor-Critic方法

**算法思想**：
结合策略梯度（Actor）和价值函数（Critic）的方法。

**算法实现**：
```python
def actor_critic(episodes, alpha_actor, alpha_critic, gamma):
    theta = np.random.randn(n_features)  # Actor参数
    w = np.random.randn(n_features)      # Critic参数
    
    for episode in episodes:
        state = env.reset()
        done = False
        
        while not done:
            # Actor选择行动
            action = select_action(state, theta)
            next_state, reward, done, _ = env.step(action)
            
            # Critic评估状态价值
            V_s = compute_value(state, w)
            V_next = compute_value(next_state, w)
            
            # TD误差
            td_error = reward + gamma * V_next - V_s
            
            # 更新Actor和Critic
            w += alpha_critic * td_error * compute_value_gradient(state, w)
            theta += alpha_actor * td_error * compute_policy_gradient(state, action, theta)
            
            state = next_state
    
    return theta, w
```

### 4.3 策略梯度方法的优缺点

**优点**：
- 可以处理连续行动空间
- 收敛性较好
- 可以学习随机策略

**缺点**：
- 样本效率较低
- 训练不稳定
- 超参数敏感

## 5. 函数逼近

### 5.1 线性函数逼近

**算法思想**：
使用线性函数来近似价值函数或Q函数。

**线性近似**：
```
V(s) ≈ φ(s)^T w
Q(s,a) ≈ φ(s,a)^T w
```

**更新规则**：
```
w ← w + α[r + γV(s') - V(s)]∇V(s)
```

### 5.2 神经网络函数逼近

**算法思想**：
使用深度神经网络来近似价值函数或策略。

**网络结构**：
- 输入层：状态特征
- 隐藏层：全连接层
- 输出层：价值函数或策略

**训练方法**：
- 使用梯度下降优化
- 经验回放缓冲区
- 目标网络稳定训练

## 6. 算法比较

### 6.1 算法特性对比

| 算法 | 模型需求 | 在线/离线 | 收敛性 | 样本效率 |
|------|---------|----------|--------|---------|
| 动态规划 | 需要 | - | 保证 | 高 |
| 蒙特卡洛 | 不需要 | 离线 | 保证 | 低 |
| TD(0) | 不需要 | 在线 | 保证 | 中等 |
| SARSA | 不需要 | 在线 | 保证 | 中等 |
| Q-Learning | 不需要 | 离线 | 保证 | 中等 |
| REINFORCE | 不需要 | 离线 | 保证 | 低 |
| Actor-Critic | 不需要 | 在线 | 保证 | 中等 |

### 6.2 适用场景

**动态规划**：
- 小规模问题
- 有完整环境模型
- 需要精确解

**蒙特卡洛**：
- 需要完整episode
- 非马尔可夫问题
- 批量学习

**时序差分**：
- 在线学习
- 连续任务
- 实时控制

**策略梯度**：
- 连续行动空间
- 随机策略
- 策略优化

## 7. 实践建议

### 7.1 算法选择
1. **小规模问题**：使用动态规划
2. **离散行动空间**：使用Q-Learning
3. **连续行动空间**：使用策略梯度
4. **在线学习**：使用SARSA或Actor-Critic
5. **离线学习**：使用Q-Learning或REINFORCE

### 7.2 超参数调优
1. **学习率**：从0.1开始，逐步调整
2. **折扣因子**：根据任务时间尺度选择
3. **探索率**：从0.1开始，逐步衰减
4. **网络结构**：根据问题复杂度选择

### 7.3 调试技巧
1. **监控奖励**：观察学习曲线
2. **可视化策略**：理解学习到的行为
3. **消融实验**：分析各组件的作用
4. **超参数搜索**：系统性地寻找最优参数
