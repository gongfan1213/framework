# 深度强化学习

## 1. 深度强化学习概述

### 1.1 什么是深度强化学习？

深度强化学习（Deep Reinforcement Learning, DRL）结合了深度学习和强化学习，使用深度神经网络来近似价值函数、策略或环境模型。

**核心思想**：
- 使用深度神经网络作为函数逼近器
- 处理高维状态空间（如图像、音频）
- 自动学习特征表示
- 端到端学习

### 1.2 深度强化学习的优势

**处理高维输入**：
- 直接从原始观察学习
- 自动特征提取
- 减少人工特征工程

**表示学习**：
- 学习层次化特征
- 捕获复杂模式
- 泛化能力强

**端到端学习**：
- 从感知到决策的完整流程
- 优化整体目标
- 减少模块间误差

### 1.3 深度强化学习的挑战

**训练稳定性**：
- 神经网络训练不稳定
- 梯度消失/爆炸
- 超参数敏感

**样本效率**：
- 需要大量交互数据
- 探索效率低
- 收敛速度慢

**泛化能力**：
- 过拟合风险
- 环境变化适应
- 迁移学习困难

## 2. 深度Q网络（DQN）

### 2.1 DQN的核心思想

DQN使用深度神经网络来近似Q函数，解决了传统Q-Learning在高维状态空间中的问题。

**网络结构**：
```python
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_dim)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

### 2.2 DQN的关键技术

**经验回放缓冲区（Experience Replay）**：
```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        return zip(*batch)
```

**目标网络（Target Network）**：
```python
class DQNAgent:
    def __init__(self, state_dim, action_dim):
        self.q_network = DQN(state_dim, action_dim)
        self.target_network = DQN(state_dim, action_dim)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
    def update_target_network(self):
        self.target_network.load_state_dict(self.q_network.state_dict())
```

### 2.3 DQN算法实现

```python
def dqn_training(env, episodes, batch_size, gamma, epsilon):
    agent = DQNAgent(state_dim, action_dim)
    replay_buffer = ReplayBuffer(10000)
    optimizer = optim.Adam(agent.q_network.parameters())
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        
        while True:
            # ε-贪婪策略选择行动
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                action = agent.q_network(state).argmax().item()
            
            # 执行行动
            next_state, reward, done, _ = env.step(action)
            replay_buffer.push(state, action, reward, next_state, done)
            
            # 训练网络
            if len(replay_buffer) > batch_size:
                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
                
                current_q_values = agent.q_network(states).gather(1, actions.unsqueeze(1))
                next_q_values = agent.target_network(next_states).max(1)[0].detach()
                target_q_values = rewards + gamma * next_q_values * (1 - dones)
                
                loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            
            state = next_state
            total_reward += reward
            
            if done:
                break
        
        # 更新目标网络
        if episode % 100 == 0:
            agent.update_target_network()
```

## 3. 深度确定性策略梯度（DDPG）

### 3.1 DDPG的核心思想

DDPG是Actor-Critic方法在连续行动空间中的扩展，使用确定性策略和深度神经网络。

**网络结构**：
```python
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(state_dim, 400)
        self.fc2 = nn.Linear(400, 300)
        self.fc3 = nn.Linear(300, action_dim)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return torch.tanh(self.fc3(x))

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim + action_dim, 400)
        self.fc2 = nn.Linear(400, 300)
        self.fc3 = nn.Linear(300, 1)
        
    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

### 3.2 DDPG算法实现

```python
class DDPGAgent:
    def __init__(self, state_dim, action_dim):
        self.actor = Actor(state_dim, action_dim)
        self.critic = Critic(state_dim, action_dim)
        self.target_actor = Actor(state_dim, action_dim)
        self.target_critic = Critic(state_dim, action_dim)
        
        # 复制参数到目标网络
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic.load_state_dict(self.critic.state_dict())
        
        self.actor_optimizer = optim.Adam(self.actor.parameters())
        self.critic_optimizer = optim.Adam(self.critic.parameters())
        
    def select_action(self, state, noise_scale=0.1):
        action = self.actor(state)
        noise = torch.randn_like(action) * noise_scale
        return torch.clamp(action + noise, -1, 1)
    
    def update(self, batch):
        states, actions, rewards, next_states, dones = batch
        
        # 更新Critic
        next_actions = self.target_actor(next_states)
        target_q = self.target_critic(next_states, next_actions)
        target_q = rewards + gamma * target_q * (1 - dones)
        current_q = self.critic(states, actions)
        critic_loss = F.mse_loss(current_q, target_q.detach())
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # 更新Actor
        actor_loss = -self.critic(states, self.actor(states)).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # 软更新目标网络
        self.soft_update(self.target_actor, self.actor, tau)
        self.soft_update(self.target_critic, self.critic, tau)
    
    def soft_update(self, target, source, tau):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
```

## 4. 近端策略优化（PPO）

### 4.1 PPO的核心思想

PPO是一种稳定的策略梯度方法，通过限制策略更新的幅度来避免训练不稳定。

**目标函数**：
```
L(θ) = E[min(r_t(θ)A_t, clip(r_t(θ), 1-ε, 1+ε)A_t)]
```
其中 r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)

### 4.2 PPO算法实现

```python
class PPOAgent:
    def __init__(self, state_dim, action_dim):
        self.policy_net = PolicyNetwork(state_dim, action_dim)
        self.value_net = ValueNetwork(state_dim)
        self.optimizer = optim.Adam([
            {'params': self.policy_net.parameters(), 'lr': 3e-4},
            {'params': self.value_net.parameters(), 'lr': 1e-3}
        ])
        
    def compute_advantages(self, rewards, values, gamma=0.99, gae_lambda=0.95):
        advantages = []
        gae = 0
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1]
            
            delta = rewards[i] + gamma * next_value - values[i]
            gae = delta + gamma * gae_lambda * gae
            advantages.insert(0, gae)
        
        return torch.tensor(advantages)
    
    def update(self, states, actions, old_log_probs, advantages, returns, epochs=10):
        for _ in range(epochs):
            # 计算新的策略概率和价值
            log_probs, values = self.policy_net(states, actions)
            ratio = torch.exp(log_probs - old_log_probs)
            
            # PPO目标函数
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 0.8, 1.2) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # 价值函数损失
            value_loss = F.mse_loss(values, returns)
            
            # 总损失
            loss = policy_loss + 0.5 * value_loss
            
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
```

## 5. 异步优势Actor-Critic（A3C）

### 5.1 A3C的核心思想

A3C使用多个并行环境来收集经验，通过异步更新来稳定训练。

**网络结构**：
```python
class A3CNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(A3CNetwork, self).__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU()
        )
        self.policy_head = nn.Linear(128, action_dim)
        self.value_head = nn.Linear(128, 1)
        
    def forward(self, state):
        shared_features = self.shared(state)
        policy = F.softmax(self.policy_head(shared_features), dim=-1)
        value = self.value_head(shared_features)
        return policy, value
```

### 5.2 A3C算法实现

```python
class A3CAgent:
    def __init__(self, state_dim, action_dim, shared_network):
        self.shared_network = shared_network
        self.optimizer = optim.Adam(self.shared_network.parameters())
        
    def compute_loss(self, states, actions, rewards, next_states, dones):
        policies, values = self.shared_network(states)
        _, next_values = self.shared_network(next_states)
        
        # 计算优势函数
        advantages = []
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0 if dones[i] else next_values[i]
            else:
                next_value = values[i + 1]
            
            advantage = rewards[i] + gamma * next_value - values[i]
            advantages.insert(0, advantage)
        
        advantages = torch.tensor(advantages)
        
        # 策略损失
        log_probs = torch.log(policies.gather(1, actions.unsqueeze(1)))
        policy_loss = -(log_probs * advantages.detach()).mean()
        
        # 价值损失
        value_loss = F.mse_loss(values, values + advantages)
        
        # 熵正则化
        entropy = -(policies * torch.log(policies)).sum(dim=1).mean()
        
        return policy_loss + 0.5 * value_loss - 0.01 * entropy
```

## 6. 软Actor-Critic（SAC）

### 6.1 SAC的核心思想

SAC是一种基于最大熵的Actor-Critic方法，通过最大化期望奖励和策略熵来学习。

**目标函数**：
```
J(π) = E[∑(r_t + αH(π(·|s_t)))]
```

### 6.2 SAC算法实现

```python
class SACAgent:
    def __init__(self, state_dim, action_dim):
        self.actor = SACActor(state_dim, action_dim)
        self.critic1 = SACCritic(state_dim, action_dim)
        self.critic2 = SACCritic(state_dim, action_dim)
        self.target_critic1 = SACCritic(state_dim, action_dim)
        self.target_critic2 = SACCritic(state_dim, action_dim)
        
        # 复制参数
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())
        
        self.actor_optimizer = optim.Adam(self.actor.parameters())
        self.critic1_optimizer = optim.Adam(self.critic1.parameters())
        self.critic2_optimizer = optim.Adam(self.critic2.parameters())
        
        self.alpha = 0.2  # 温度参数
        
    def select_action(self, state):
        action, log_prob = self.actor.sample(state)
        return action, log_prob
    
    def update(self, batch):
        states, actions, rewards, next_states, dones = batch
        
        # 更新Critic
        next_actions, next_log_probs = self.actor.sample(next_states)
        target_q1 = self.target_critic1(next_states, next_actions)
        target_q2 = self.target_critic2(next_states, next_actions)
        target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
        target_q = rewards + gamma * target_q * (1 - dones)
        
        current_q1 = self.critic1(states, actions)
        current_q2 = self.critic2(states, actions)
        
        critic1_loss = F.mse_loss(current_q1, target_q.detach())
        critic2_loss = F.mse_loss(current_q2, target_q.detach())
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        self.critic1_optimizer.step()
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        self.critic2_optimizer.step()
        
        # 更新Actor
        new_actions, new_log_probs = self.actor.sample(states)
        q1_new = self.critic1(states, new_actions)
        q2_new = self.critic2(states, new_actions)
        q_new = torch.min(q1_new, q2_new)
        
        actor_loss = (self.alpha * new_log_probs - q_new).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # 软更新目标网络
        self.soft_update(self.target_critic1, self.critic1)
        self.soft_update(self.target_critic2, self.critic2)
```

## 7. 深度强化学习的应用

### 7.1 游戏AI
- **Atari游戏**：DQN在Atari 2600游戏上的突破
- **围棋**：AlphaGo、AlphaZero
- **星际争霸**：AlphaStar
- **Dota 2**：OpenAI Five

### 7.2 机器人控制
- **机械臂控制**：抓取、装配任务
- **移动机器人**：导航、避障
- **无人机控制**：飞行控制、路径规划

### 7.3 自动驾驶
- **路径规划**：最优路径选择
- **行为决策**：变道、超车决策
- **交通流优化**：信号灯控制

### 7.4 推荐系统
- **个性化推荐**：用户兴趣建模
- **A/B测试优化**：策略选择
- **资源分配**：广告投放优化

## 8. 实践建议

### 8.1 环境选择
1. **简单环境**：CartPole、MountainCar
2. **中等复杂度**：LunarLander、Pendulum
3. **复杂环境**：Atari游戏、MuJoCo环境

### 8.2 超参数调优
1. **学习率**：从3e-4开始调整
2. **网络结构**：根据问题复杂度选择
3. **批次大小**：32-256之间选择
4. **探索参数**：根据算法调整

### 8.3 训练技巧
1. **梯度裁剪**：防止梯度爆炸
2. **学习率调度**：逐步降低学习率
3. **早停机制**：防止过拟合
4. **模型保存**：保存最佳模型

### 8.4 调试方法
1. **监控指标**：奖励、损失、梯度范数
2. **可视化**：学习曲线、策略可视化
3. **消融实验**：分析各组件作用
4. **超参数搜索**：系统化调优
