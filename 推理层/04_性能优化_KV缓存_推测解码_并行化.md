### 04 性能优化：KV缓存、推测解码与并行化

本篇聚焦核心性能手段，直击TTFT与TPOT（tokens per output time）。

#### KV Cache（注意力键值缓存）
- 原理：自回归解码时复用历史层输出，避免重复计算；对长上下文至关重要。
- 策略：
  - 共享前缀：对相同系统提示/公共Prompt复用缓存。
  - 分页化（PagedAttention/PagedKV）：减少碎片化、支持多请求并发。
  - 生命周期与回收：LRU/TTL/租户优先级；内存水位触发清理。

#### 推测解码（Speculative Decoding）
- 思路：用“草稿模型”或“多步并行”预测多个候选tokens，主模型快速验证，减少等待。
- 变体：
  - Draft Model（小模型）+ Target Model（大模型）协同。
  - Medusa、EAGLE 等多头/多分支并行推断结构。
- 适用：温度较低、分布较尖锐时效果更好；需平衡拒绝率与验证开销。

#### 并行化策略
- 张量并行（TP）：切分权重张量；适合大显存占用的单层矩阵。
- 流水并行（PP）：切分层到不同GPU，适合超大深度网络。
- 数据并行（DP）：多副本并发处理不同请求，结合批处理。
- 专家并行（MoE）：动态路由到部分专家，提升参数规模与吞吐比。

#### 采样与输出优化
- 减少过多采样逻辑在CPU执行；尽量GPU端采样或内核融合。
- 流式SSE/gRPC分块，低首包延迟（TTFT），提升主观体验。

#### I/O与拷贝优化
- 避免CPU-GPU频繁往返；Pinned Memory与异步DMA；尽量就地（in-place）.

#### 真实场景建议
- 优先上KV分页与动态批处理；在流量稳定后再引入推测解码；评估性价比后决定量化与并行深度。


