### 14 中国市场 GPU 选型与部署建议（面向推理）

面向中国常见可获得的 GPU 型号，给出推理层选型建议与场景匹配。以下为通用经验，实际以你的模型规模、上下文长度、SLA 为准。

#### 常见可选 GPU（2024-2025）
- NVIDIA A800 80GB：算力接近 A100 的国内替代方案，NVLink 互联，适合中大型模型高并发推理与多卡并行；生态成熟（vLLM/TensorRT-LLM 适配好）。
- NVIDIA H20 96GB：H100 系列国内替代，显存更大、BF16/FP8 性能强，追求极致吞吐/低延迟、长上下文的优选。
- NVIDIA L20 48GB：面向推理的高性价比，适合7B/13B级模型与中等上下文。
- NVIDIA L40S 48GB：图形/AI通用卡，推理性价比可观，适合中小模型与多任务混部。
- NVIDIA A30 24GB / A10 24GB：存量较多，适合小模型/轻负载。

注：国产 GPU（如昇腾、寒武纪、壁仞等）生态在快速发展，但与主流 LLM 推理栈（vLLM、TensorRT-LLM）适配程度各异，需结合厂商 SDK 与模型支持度评估。

#### 选型建议
- 模型规模：
  - 7B/13B：L20/L40S 单卡可用；高并发建议 A800；
  - 30B/70B：倾向 A800/H20，多卡并行（TP/PP）。
- 上下文长度：
  - ≤8K：48GB 显存一般足够；
  - 32K-128K：建议 80GB/96GB，并启用 PagedAttention 与 CPU/NVMe 交换。
- 目标：
  - 低延迟交互：优先更高频宽与更大显存（A800/H20），控制批大小；
  - 高吞吐离线：更大批处理、更多副本、L20/L40S 也可性价比更高。

#### 配置建议
- 驱动与 CUDA：保持与 vLLM/TensorRT-LLM 支持矩阵匹配；
- 拓扑：优先 NVLink 相连卡做 TP/PP；
- 容器：只读权重缓存+本地 NVMe 加速权重拉取；
- 监控：GPU 利用率、显存水位、P95/P99 延迟，动态扩缩容。

#### 采购与成本
- 公有云 vs 自建：云上弹性好、成本略高；自建前期投入大、单位成本低；
- 成本核算：$/token、$/req、SLA失败成本、夜间降配策略；结合量化/推测解码评估。


