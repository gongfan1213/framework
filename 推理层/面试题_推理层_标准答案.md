### 推理层面试题与标准答案

以下题目覆盖概念、工程、优化与实战。每题后附标准答案要点，供自测与面试演练。

#### 一、概念与总体设计
1) 什么是推理层？与训练/微调阶段的工程侧重点有何不同？
- 参考答案要点：承载在线/离线推断的工程体系；推理关注低时延、稳定SLA、弹性与成本；训练关注吞吐与收敛；手段不同（KV缓存/批处理/量化/并行 vs 数据并行/优化器/Checkpoint）。

2) 解释TTFT与TPOT含义及其优化手段。
- 要点：TTFT首字节延迟；TPOT为生成吞吐（tokens/s）。优化：KV缓存、动态批处理、PagedAttention、GPU端采样、推测解码、量化与并行。

3) 推理服务的关键SLO/SLA指标有哪些？
- 要点：可用性、错误率、P50/P95/P99延迟、TTFT、吞吐、成本、稳定性（抖动）、降级比例。

#### 二、架构与组件
4) 画出典型推理服务架构并说明各层职责。
- 要点：接入层、编排层、推理服务层、加速与内存层、资源/集群层、观测与治理层。

5) 动态批处理与持续批处理的区别与取舍？
- 要点：动态批按窗口合并；持续批在解码中吸收新请求；吞吐↑但尾延迟可能↑；需与SLA权衡。

6) 为什么PagedAttention能提升并发与显存效率？
- 要点：分页管理KV，减少碎片，支持跨请求共享与快速回收，缓解长序列显存压力。

7) 如何设计多模型多版本的灰度发布与回滚？
- 要点：权重路由、金丝雀比例、健康阈值、自动回滚、特性开关、可重放日志验证。

#### 三、性能优化
8) 讲解KV缓存的工作原理、共享前缀与回收策略。
- 要点：自回归重用历史键值；相同Prompt共享；LRU/TTL/优先级回收，水位触发清理。

9) 推测解码的核心思想、适用条件与潜在问题？
- 要点：草稿模型或多步并行预测，主模型验证；低温度更有效；拒绝率与验证开销权衡，稳定性与一致性风险。

10) 在GPU/CPU拷贝与采样阶段有哪些常见性能陷阱？
- 要点：频繁host-device往返、未使用Pinned Memory、CPU端采样瓶颈、无内核融合、同步阻塞。

11) 张量并行、流水并行、专家并行分别适合什么场景？
- 要点：TP适合巨矩阵、PP适合深网络超大模型、MoE适合扩大参数与性价比；注意通信与负载均衡。

#### 四、部署与稳定性
12) 生产中如何做限流、熔断与降级？
- 要点：租户QPS/Token限额、排队与超时；熔断保护后端；降级为小模型/缩短上下文/降低温度。

13) 如何进行容量规划与弹性伸缩？
- 要点：基于历史峰值与突发，结合队列长度/GPU利用率触发扩缩；预热容器与权重缓存，避免冷启动抖动。

14) 如何设计可观测性以快速定位端到端瓶颈？
- 要点：统一Trace贯穿接入/编排/推理；Metrics+Logs+Profile；按请求与模型维度切片分析。

#### 五、成本与安全
15) 成本优化的主要抓手有哪些？
- 要点：量化、批处理、KV共享、合适GPU与并行深度、分时调度、按租户计费与配额。

16) 如何防御提示注入与越狱？
- 要点：输入过滤、提示分隔与不可见指令、策略模型或规则引擎审核、工具调用白名单。

#### 六、综合题（开放题）
17) 给定一个P95时延过高的服务，如何定位与优化？
- 要点：拆解TTFT与token速率；观测排队与批大小；核查采样位置与CPU瓶颈；KV分页与分页大小；尝试vLLM/TensorRT-LLM；压测与A/B验证。

18) 如何设计一套A/B实验来评估推测解码收益？
- 要点：固定Prompt/温度，控制变量；记录拒绝率、TTFT、tokens/s、质量指标；统计显著性检验，回滚条件。


