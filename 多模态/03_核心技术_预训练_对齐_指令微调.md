### 03 核心技术：预训练、对齐、指令微调

#### 视觉与语言对齐
- 视觉编码器输出特征（patch embedding/CLS）通过投影层映射到LLM token维度；
- 冻结LLM + 训练适配器/LoRA 以低成本实现对齐；
- 对比学习（CLIP式）与生成式监督（图像描述）结合。

#### 指令微调（SFT）
- 多模态指令数据：图像-问题-答案/解析步骤，提升模型遵循指令与推理能力；
- 数据来源：人类标注、合成数据（描述/QA）、OCR/表格/图表专用数据。

#### 专项能力
- OCR/图表/表格：加入结构化提示、坐标/token索引、版面分析；
- 工具使用：让模型学会在视觉上下文中调用OCR、检测、检索等外部工具。

#### 长上下文与高分辨率
- 分块/金字塔（multi-scale）编码；
- 选帧/采样降低视频token负担；
- PagedAttention与外部存储（CPU/NVMe）配合长视频。


