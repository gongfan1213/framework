### 07 多模态原理（通俗易懂版）

本篇从“图片/音频/视频是如何被大模型理解的”出发，循序渐进讲清核心概念、训练与推理流程、架构拼装方式与工程取舍。读完后，你应能用自己的话解释：多模态如何把不同感官的信息变成“语言模型能理解并推理的代号”，并知道常见方案的优缺点与适用场景。

---

#### 1. 模态是什么？为什么要“对齐”？
- 模态（Modality）= 信息的“形态”：文字、图像、音频、视频、传感器数据…
- 语言模型（LLM）最擅长处理“离散的词（token）”。
- 图片/音频是“连续信号”，直接喂给LLM不合适，需要先“翻译成LLM能懂的符号”。
- 对齐（Alignment）= 把视觉/音频的“连贯特征”映射到语言世界的“离散表示”，让LLM把这些表示当作“特殊词”来推理。

简单比喻：
- LLM是一个“会思考的读者”，但只看得懂“文字”。
- 视觉编码器把图片拆成“有意义的词组”（视觉token），再经投影器把这些“词组”翻译成LLM词典能理解的“外来词”。

---

#### 2. 从像素到“词”：视觉是如何被编码的？
以图片为例：
1) 切块与嵌入：把图像分成小块（如16×16 patch），每块做线性变换得到向量（patch embedding）。
2) 视觉Transformer（ViT）：像读一段话一样读这些patch向量，建模全局关系，输出一串“视觉特征向量”。
3) 投影（Projector/Adapter/Q-Former）：把视觉特征的维度/语义映射到LLM的隐藏维度，得到“视觉token”。
4) 串接到LLM：把“视觉token”与用户文字一起送入LLM，像“带图对话”一样进行推理与生成。

音频/视频类似：
- 音频用声学编码器（如Conformer/Whisper编码器）得到音频特征，再投影给LLM。
- 视频在图像基础上加“时间维度”，常见做法是抽帧+时序建模（TimeSformer/时序池化），控制token数量。

---

#### 3. 为什么不直接把像素当成token？
- 直接像素→LLM 会极其低效：输入长度爆炸、难以学习长程视觉结构。
- 先“压缩”为视觉特征（高层语义）更高效，就像把一本图画书的场景先总结成关键描述再交给作家创作。

---

#### 4. 对齐与训练目标：模型如何学会“看图说话”？
- 对比学习（CLIP思路）：让“图片-文字配对”更靠近，“不匹配”更远，学会图文语义的“对应关系”。
- 生成式学习（Caption/QA）：给图与描述/问答，让模型学会用语言“解释”图像。
- 指令微调（SFT）：把真实任务写成“指令+输入+期望输出”的格式，教模型遵循指令、完成多步推理。
- 进阶：加入OCR/表格/图表/代码/工具使用等专项数据，提升垂直能力。

---

#### 5. 推理时到底发生了什么？（以图文问答为例）
1) 输入：用户问题 + 图片。
2) 视觉侧：图片→patch→ViT→视觉特征→投影→视觉token。
3) 串接：把视觉token放在系统提示或用户问题前后，一起喂给LLM。
4) LLM推理：把视觉token当“特殊词”，结合问题做自回归生成，输出答案。
5) 流式输出：像打字一样吐出答案，提升体验（TTFT低，逐字显示）。

---

#### 6. 典型架构拼装法
- 视觉编码器 + 投影器 + 冻结LLM：
  - 成本低，稳定；通过LoRA/Adapter训练投影器即可。
- 视觉编码器 + 轻微微调LLM顶层：
  - 质量更好，成本更高；需注意灾难性遗忘与漂移。
- 统一Transformer（端到端）：
  - 省去投影，但训练成本和数据要求高；更“整洁”，趋势路线之一。

---

#### 7. 多模态为什么“容易卡显存”？
- 每张高分辨率图像就像很多“视觉token”；视频等价于“超长上下文”。
- 上下文越长，注意力/KV缓存越大，显存占用上升。
- 工程解法：
  - 控制分辨率/抽帧/多尺度（先低清全局，再高清局部）。
  - 分桶批处理（按分辨率/长度分组），减少浪费。
  - PagedAttention（分页KV）、CPU/NVMe交换，长上下文更稳。

---

#### 8. 常见能力与难点
- OCR/表格/图表：需要结构化能力，通常配合专门数据或工具。
- 视觉定位与操作：需要在描述中定位UI元素（文本/坐标/风格），可配合检测/分割模型。
- 视频理解：时序关系关键，抽帧策略影响大；推理代价高。
- 语音：ASR/对话/翻译/合成链路中每一环的误差都会传播。

---

#### 9. 与Agent结合：让“看见”指导“行动”
经典闭环：Perceive（感知）→ Plan（计划）→ Act（执行）→ Verify（验证）
- 感知：截图/相机帧→视觉编码→OCR/检索→得到“可读摘要”。
- 计划：LLM基于摘要与目标拆解步骤（少步、可验证）。
- 执行：浏览器/机器人/脚本工具按步骤操作。
- 验证：再截图或抓取页面文本，对比预期；必要时重试或人工介入。

工程建议：
- 先用OCR把视觉转文本再计划，稳定简单；
- 进阶再接入VLM（图像+文本原生推理）提升理解效果；
- 关键路径上做流式输出与可观测性（trace+metrics）。

---

#### 10. 代表性模型与选型思路（简版）
- Qwen2-VL / InternVL：中文与OCR/表格强，多场景通用。
- LLaVA / BLIP-2：社区成熟、生态丰富，英文/通用场景好。
- Whisper：高质量ASR前端，常与LLM结合做语音对话。
- 视频模型：Qwen2-VL-Video / LLaVA-Video，用于时序理解。

选型经验：
- 中文+文档/表格：优先Qwen2-VL与国内生态模型；
- 资源受限：采用“视觉编码器+冻结LLM+小适配器”的方案；
- 长上下文/高分辨率：选择支持PagedAttention与分桶策略的推理栈（如vLLM）。

---

#### 11. 常见问题（FAQ）
Q1：为什么同一张图，模型有时答得好、有时不好？
- A：提示与画面裁剪、分辨率、OCR质量、模型温度等都会影响；多试提示模板、固定温度和抽帧策略，质控更稳。

Q2：多模态一定要上VLM吗？
- A：不一定。很多业务用“截图+OCR+LLM文本推理”更稳定省显存；VLM适合更复杂的视觉语义任务。

Q3：为什么视频很贵？
- A：视频≈很多张图+时间关系，token数量级暴涨；需抽帧、分段与层次化建模。

---

#### 12. 一页总结（给非专业同学）
- 多模态=把图/音/视频翻译成“词”，再交给“会思考的读者”（LLM）。
- 关键环节：编码（提炼要点）→ 对齐（翻译成LLM能懂的表示）→ 指令微调（教它用人类方式做题）。
- 工程重点：控制“长度与分辨率”、批处理与显存管理、必要时引入OCR/工具链；与Agent闭环可实现“看→想→做→验”。


