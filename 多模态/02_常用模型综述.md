### 02 常用模型综述

#### 视觉-语言（VLM）代表
- CLIP：对比学习对齐图文；用于检索/零样本分类/提示条件。
- BLIP/BLIP-2：图像到文本生成与指令化；BLIP-2通过Q-Former桥接视觉与LLM。
- LLaVA/LLaVA-Next：ViT编码+适配器接入LLM，多轮视觉对话代表。
- Qwen-VL/Qwen2-VL：多分辨率图像理解、强OCR/表格能力，适合中文场景。
- InternVL/InternLM-XComposer：国内开源体系，图文推理与问答强。
- Kosmos、MiniGPT-4、mPLUG-Owl：多模态指令优化的多种探索路线。

#### 语音/音频相关
- Whisper：高质量ASR，常用作语音到文本前端；
- SeamlessM4T/SpeechT5：端到端语音翻译/合成框架；
- AudioLDM：文本到音频生成。

#### 视频相关
- Video-VL：视频帧编码 + 时序建模；
- LLaVA-Video、Qwen2-VL-Video：在图像VLM基础上扩展时间维度；
- Sora（闭源趋势）：文本到视频生成展示出端到端生成能力。

#### 模型选择建议（实用导向）
- 中文与OCR/表格强：Qwen2-VL、InternVL。
- 通用英文/多语言：LLaVA-Next、BLIP-2 家族。
- 语音前端：Whisper（配合任意LLM）。
- 视频理解：Qwen2-VL-Video / LLaVA-Video（看推理支持与算力）。


